Model, Mode,Server, Data_Type, Use_Case, Batch_Size, Result
fastrcnn;training;CLX;fp32;Latency;1;363.497;benchmark_fastrcnn_training_aipg-ra-clx-88_throughput.log;Private-TensorFlow-Benchmark-Q2-Py2-Master-CLX;293
fastrcnn;training;CLX;fp32;Throughput;1;2.75;benchmark_fastrcnn_training_aipg-ra-clx-88_throughput.log;Private-TensorFlow-Benchmark-Q2-Py2-Master-CLX;293
Wide_Deep_Census;training;CLX;fp32;Accuracy;40;1.01.0;benchmark_Wide_Deep_Census_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312
Wide_Deep_Census;training;CLX;fp32;Throughput;40;10020.9602955;benchmark_Wide_Deep_Census_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312
Wide_Deep_Criteo;training;CLX;fp32;Accuracy;128;;benchmark_Wide_Deep_Criteo_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312
Wide_Deep_Criteo;training;CLX;fp32;Throughput;128;;benchmark_Wide_Deep_Criteo_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312
MobileNet_v1;training;CLX;fp32;Throughput;64;189.4507;benchmark_MobileNet_v1_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312
Inception_ResNet_v2;training;CLX;fp32;Throughput;64;11.8954;benchmark_Inception_ResNet_v2_training_aipg-ra-clx-90_throughput.log;Private-TensorFlow-Benchmark-Q3-FP32-Models-CLX;312


faster_rcnn;inference;CLX;fp32;Accuracy;1;0.316,0.489,;faster_rcnn/benchmark_faster_rcnn_inference_fp32_accuracy_py2_aipg-ra-clx-91.log;Intel-Models-Benchmark-fp32-Trigger;296
faster_rcnn;inference;CLX;fp32;Latency;1;181;faster_rcnn/benchmark_faster_rcnn_inference_fp32_latency_py2_aipg-ra-clx-91.log;Intel-Models-Benchmark-fp32-Trigger;296
faster_rcnn;inference;CLX;fp32;Throughput;1;5.52;faster_rcnn/benchmark_faster_rcnn_inference_fp32_latency_py2_aipg-ra-clx-91.log;Intel-Models-Benchmark-fp32-Trigger;296
faster_rcnn;inference;CLX;int8;Accuracy;1;0.309,0.477,;faster_rcnn/benchmark_faster_rcnn_inference_int8_accuracy_py2_aipg-ra-clx-105.log;Intel-Models-Benchmark-int8-Trigger;79
faster_rcnn;inference;CLX;int8;Latency;1;57.242;faster_rcnn/benchmark_faster_rcnn_inference_int8_latency_py2_aipg-ra-clx-105.log;Intel-Models-Benchmark-int8-Trigger;79
faster_rcnn;inference;CLX;int8;Throughput;1;17.47;faster_rcnn/benchmark_faster_rcnn_inference_int8_latency_py2_aipg-ra-clx-105.log;Intel-Models-Benchmark-int8-Trigger;79


wide_deep;inference;CLX;fp32;Latency;1;1.16;wide_deep/benchmark_wide_deep_inference_fp32_latency_py2_aipg-ra-clx-109.log;Intel-Models-Benchmark-fp32-Trigger;296
wide_deep;inference;CLX;fp32;Throughput;1024;4016.7420914;wide_deep/benchmark_wide_deep_inference_fp32_throughput_py2_aipg-ra-clx-109.log;Intel-Models-Benchmark-fp32-Trigger;296
